- Towards a practice of token engineering #learn #tokenomics
	- https://getpocket.com/read/2094298133#:~:text=23%20min-,View%20Original,-Methodology%2C%20Patterns%20%26%20Tools
	- We can frame **token design as optimization design**, then use optimization design methodology.
	- **Simulation, verification, and design space exploration (CAD tools) **for circuit design have helped engineers analyze, design, and verify wickedly complex chips. We can look forward to similar tools for tokenized ecosystems.
	- When you have an appropriately timed force applied to a system in resonance, the amplitude of the resonance *grows *over time.
	- However, it turns out that if you zoom in on Mechanism Design with a [few practical constraints](https://medium.com/blockchannel/a-crash-course-in-mechanism-design-for-cryptoeconomic-applications-a9f06ab6a976), you end up with Optimization Design! People doing Optimization Design have a tremendous amount of practical experience deploying optimizer systems over the years.
	- ![image.png](../assets/image_1666424860878_0.png)
	- ![image.png](../assets/image_1666424870213_0.png)
	- ![image.png](../assets/image_1666424887993_0.png)
	- Token design is like optimization design: at a high level, you encode intent with a block rewards function aka objective function, and you let it fly. As is often the case, [Simon de la Rouviere](https://medium.com/@simondlr) [saw this one first](https://hackernoon.com/history-is-rhyming-fitness-functions-comparing-blockchain-tokens-to-the-web-3c117239f4c).
	- It gets more specific than that. Token design is *especially *like evolutionary algorithms (EAs), where there are many agents “searching” at once and there is no top-down control of what each agent does. Agents live and die by their block reward or fitness.
	- With such similarities, *we can use best practices from optimization / EA to when doing token design*. This is great news, because many people are Jedis in designing EAs and optimization systems.
		- ![image.png](../assets/image_1666424926205_0.png)
		- Goals
			- Both tokenized ecosystems and EAs have **goals**, in the form of **objectives **(things to maximize or minimize) and **constraints **(things that must be met). To get fancy, this can even be stochastic.
			- The tokenized ecosystem might give block rewards for an objective function of “maximize hash rate” whereas an EA objective might be “minimize error” in training a deep net. Constraints might be “must have stake ≥threshold to participate” or “deep network layers=100” respectively.
			- Variants include single-objective optimization (1 objective, 0 constraints), constraint satisfaction (0 objectives, ≥1 constraints), and multi-objective constrained optimization (≥2 objectives, ≥1 constraints).
		- Measurement and test
			- To **test / measure **success against the goals (objectives & constraints), a tokenized ecosystem relies on **proofs **and an optimizer measures **fitness **using e.g. a **simulator**.
			- For example, a Bitcoin node proves that a user was hashing by verifying that the user’s supplied nonce solves the the cryptographic puzzle.
			- An optimizer might test the goodness of a circuit by running a SPICE simulation of the circuit’s differential equations; simulation results can be verified by testing whether they indeed solved Kirchoff’s Current and Voltage Laws.
		- System agents
			- In both systems, **agents **run about “doing things”.
			- In a tokenized ecosystem, **network stakeholders **such as **miners (**or **users **more generally) do whatever it takes to earn block rewards. They jostle about, doing what it takes to get more token rewards. For example, in Bitcoin some agents might design, build, and run ASIC chips to get higher hash rate. Other agents might pool their existing compute resources. The system does not need to explicitly model all stakeholders in the ecosystem. For example, Bitcoin doesn’t have specific roles for banks or nations or companies; it’s mostly all about the miners.
			- In an EA, you have **individuals **in a** population. **If they’re “good” they have higher fitness. For example, an individual may be a vector of 10,000 weights for a neural network. “Actions” of individuals are basically when they survive and have variants made of them, via operators like crossover (e.g. interpolation) or mutation (e.g. randomly perturbing each parameter).
		- System clock
			- Each system has a **clock**, implying a **time dimension** by which **progress **is made / **convergence **is happening.
			- **Batches. **Typically, agents are processed in batches or epochs. A tokenized ecosystem periodically generates a **new block** and gives block rewards. The new block points to the old block; and new work in the system will add to the new block; and so on. This linked list of blocks implies a Lamport-style logical clock. In EAs, each batch is a **generation **where a whole population of individuals gets updated at once. Each generational loop might include: evaluate individuals, select the best, let them make children, repeat.
			- **Continuous.** In some systems, agents are processed more **continuously** rather than batches. These systems usually takes a bit more work to conceptualize, but may lead to better properties for some problems. For example in tokenized ecosystems, a Stellar transaction only needs validation from quorum slice participants, or another node gets added to a DAG (directed acyclic graph) like in Iota. In EAs, we have steady-state evolution where one individual at a time is replaced.
		- Incentives and disincentives
			- The system itself cannot control how the agents behave. (Or at the very least, it shouldn’t *need *to control them.) As such, **top-level behavior must be an emergent property of bottom-up actions by agents**. This is necessary for tokenized ecosystems; otherwise they’d be centralized! It’s not an absolute must for EAs, but nonetheless a broad set of EAs take this approach for simplicity / elegance or meeting other design goals.
			- This means the system can only reward or punish behavior, aka carrots or sticks, aka **incentives **and **disincentives**. In designing the system, we *design* what rewards or punishments to give, and how to give them.
			- In tokenized ecosystems, rewards take the form of block rewards, and punishments by slashing stake. The former is typically the objective function; the latter is some (but not all) constraints.
			- In EAs, reward and punishment both come down to which individuals are selected to be parents for the next generation. Examples: randomly choose two individuals and keep the best, repeating until full (tournament selection); and chance of selection is proportional to fitness (roulette wheel selection). Crucially, the EA does *not *need to steer the individuals by e.g. providing a derivative. This is why a tokenized ecosystem is most like an EA, versus gradient-based optimizers that give top-down directives (using gradients to choose new individuals).
	- But practitioners of the algorithms all do something very similar. They want to ship optimizer systems that *just work*. They follow the following steps. Some do it implicitly, though the pros do it systematically:
		- **Formulate the problem:** They assume that the algorithm “just works” and they focus on formulating the problem in terms of** objectives and constraints **(goal) and design space (where can the optimizer explore, which is really just constraints).
		- **Try an existing solver: **Then they run the algorithm against those goals and let it “solve”. Code for optimization algorithms are often simply called “**solvers**”. If this doesn’t work, practitioners will iterate by trying different problem formulations, or different solvers and solver parameters.
		- **New solver? **If the previous solving step doesn’t work, even after repeated tries on various formulations, then practitioners consider rolling their own solver, i.e. designing a new optimization algorithm.
	-
-